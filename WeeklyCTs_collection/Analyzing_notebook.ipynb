{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, I will describe all the steps that I have taken to make a new dataset for Weekly CTs.\n",
    "\n",
    "Basically, the process contains five different steps:\n",
    "\n",
    "1. Navigation of the folder in which one think there maybe any weeklyCTs. These folders can be on this computer or a user can just make these folders by downloading new patients from MIRADA or other UMCG datasets.\n",
    "\n",
    "2. Extracting only weeklyCTs from these folders and make an excel file from them.\n",
    "\n",
    "3. Transferring the new-founded weeklyCTs into a destination folder (it can be an existing folder for the weeklyCTs or a new folder).\n",
    "\n",
    "4. Making a report excel file of some information about the weeklyCTs in the destination file and some clinical information from the patients who have these weeklyCTs.\n",
    "\n",
    "5. Making a pannel that contains different information about the WeeklyCT dataset.\n",
    "\n",
    "6. A Watchdog is keep the track of all the additions to the destination folder, and save them in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/gridstack/gridstack@4.2.5/dist/gridstack-h5.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://cdn.holoviz.org/panel/0.14.4/dist/css/alerts.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/card.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/dataframe.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/debugger.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/json.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/loading.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/markdown.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/widgets.css\"];\n  var inline_js = [    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arc:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n    },    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.bk-root, .bk-root .bk:before, .bk-root .bk:after {\n",
       "  font-family: var(--jp-ui-font-size1);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Libraries\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import shutil\n",
    "import panel as pn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from datetime import time, datetime, date\n",
    "\n",
    "# DICOM Libraries\n",
    "import pydicom as pdcm\n",
    "from pydicom.tag import Tag\n",
    "\n",
    "\n",
    "# Bokeh libraries\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import ColumnDataSource\n",
    "from bokeh.models import MultiChoice, LabelSet\n",
    "from bokeh.io import output_notebook, output_file\n",
    "from bokeh.plotting import figure, show, row, reset_output\n",
    "\n",
    "# Activate bokeh output and panel extension\n",
    "output_notebook()\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Navigation Phase\n",
    "### DICOM Files\n",
    "All kinds of CTs were stored in the form of DICOM files. DICOM, which stands for Digital Imaging and Communications in Medicine, is a standard for transmitting, storing, and sharing medical images. DICOM files contain information about medical images, such as X-rays, CT scans, MRIs, and ultrasound. This standard ensures the interoperability of medical imaging equipment from different manufacturers. Some key features are:\n",
    "\n",
    "**Metadata:** DICOM files store not only the pixel data of the medical images but also a wealth of metadata. This metadata includes patient information, imaging device details, acquisition parameters, and more.\n",
    "\n",
    "**Interoperability:** DICOM enables the exchange of medical images and related information between different devices and systems. This interoperability is crucial in healthcare settings where various imaging modalities and equipment are used.\n",
    "\n",
    "**Structured Data:** DICOM files use a structured format for information, allowing for consistency and ease of interpretation by different systems. This makes it possible for healthcare professionals to access and understand the data regardless of the equipment used to capture or generate the images.\n",
    "\n",
    "For information of different tags and the definitions one can use the following links: [Wiki](https://en.wikipedia.org/wiki/DICOM), [link](https://dicom.innolitics.com/ciods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_name(image, subf):\n",
    "\n",
    "    # find the name of the folder\n",
    "    try:\n",
    "        folder_name = image[Tag(0x0008103e)].value\n",
    "\n",
    "    except:\n",
    "        study = image[Tag(0x00081030)].value\n",
    "        patient_id = image[Tag(0x00100020)].value\n",
    "        print(f'Warning: folder {study} with {patient_id} ID does NOT have Series Description')\n",
    "        folder_name = subf.split('\\\\')[-1]  \n",
    "\n",
    "    return folder_name\n",
    "\n",
    "def get_patient_id(image):\n",
    "\n",
    "    # Extract the patient ID\n",
    "    try:\n",
    "        patient_id = int(image[Tag(0x00100020)].value)\n",
    "\n",
    "    except:\n",
    "        print(f'Warning: There is NO patient ID')\n",
    "        patient_id = None\n",
    "\n",
    "    return patient_id\n",
    "\n",
    "def get_probable_weklyct_name(name, number, names_list, saver):\n",
    "\n",
    "    lowercase_name = name.lower()\n",
    "\n",
    "    # Search to find 'rct' or 'w' with a number\n",
    "    if ('rct' in lowercase_name or 'w' in lowercase_name) and re.search(r'\\d', name):\n",
    "        saver = name\n",
    "\n",
    "    elif 'wk..' in lowercase_name and not re.search(r'\\d', name):\n",
    "        saver = name\n",
    "\n",
    "    # Check if 'w' is in 'j' and the next element in 'sep_names' is an integer\n",
    "    elif 'w' in lowercase_name and number + 1 < len(names_list) and not re.search(r'\\d', name):\n",
    "\n",
    "        if '2.0' not in names_list[number + 1] and '2,' not in names_list[number + 1]:\n",
    "            saver = name + str(names_list[number + 1])\n",
    "\n",
    "    elif re.search('rct.*[..]|rct.*[#]', lowercase_name) and not re.search(r'\\d', name):\n",
    "        saver = name\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return saver    \n",
    "    \n",
    "def get_hd_fov(name, hd_fov):\n",
    "\n",
    "    lowercase_name = name.lower()\n",
    "    # Search whether there is 'hd' or 'fov' in j\n",
    "    if 'hd' in lowercase_name or 'fov' in lowercase_name:\n",
    "        hd_fov = 1 \n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return hd_fov\n",
    "\n",
    "def get_fraction(name, fraction):\n",
    "\n",
    "    lowercase_name = name.lower()\n",
    "\n",
    "    # Find the fraction number\n",
    "    if 'rct' in lowercase_name and re.search(r'\\d', name):\n",
    "        fraction = int(re.findall(r'\\d+', name)[0])\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return fraction\n",
    "\n",
    "def get_date_information(image):\n",
    "\n",
    "    # Extract the date, the week day, and the week number from study date time\n",
    "    try:\n",
    "        study_datetime_CT = datetime.strptime(image[Tag(0x00080020)].value ,\"%Y%m%d\")\n",
    "        date_info = study_datetime_CT.date()\n",
    "        weekday = study_datetime_CT.weekday() + 1\n",
    "        week_num = study_datetime_CT.isocalendar()[1] #week\n",
    "\n",
    "    except:\n",
    "        date_info = None\n",
    "        weekday = None\n",
    "        week_num = None \n",
    "    \n",
    "    return date_info, weekday, week_num\n",
    "\n",
    "def get_slice_thickness(image):\n",
    "    \n",
    "    # Extract slice thickness\n",
    "    try:\n",
    "        slice_thickness = image['00180050'].value\n",
    "    except:\n",
    "        slice_thickness = None\n",
    "    \n",
    "    return slice_thickness\n",
    "\n",
    "def get_contrast(image):\n",
    "    \n",
    "    # Extract contrast information\n",
    "    try:\n",
    "        image[Tag(0x00180010)].value\n",
    "        contrast=1\n",
    "\n",
    "    except:\n",
    "        contrast=0\n",
    "    \n",
    "    return contrast\n",
    "\n",
    "def get_pixel_spacing(image):\n",
    "\n",
    "    # Extract pixel spacing\n",
    "    try:\n",
    "        pixel_spacing = image[Tag(0x00280030)].value\n",
    "    except:\n",
    "        pixel_spacing = None\n",
    "    \n",
    "    return pixel_spacing\n",
    "\n",
    "def get_ref_uid(image):\n",
    "\n",
    "    # Extract UID\n",
    "    try:\n",
    "        uid = image['00200052'].value\n",
    "    except:\n",
    "        uid = None\n",
    "    \n",
    "    return uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_folder(path_folder, output_path, file_name):\n",
    "\n",
    "    # Add in config\n",
    "    exclusion_set = {'detail', 'ac_ct', 'ld_ct', 'ld ct', 'ac ct'} # CONFIG File\n",
    "    min_slice_num = 50 # CONFIG File\n",
    "    modality = 'CT' # CONFIG File\n",
    "\n",
    "    # Make a group to save all the information\n",
    "    group = list()\n",
    "\n",
    "    for r, d, f in os.walk(path_folder):\n",
    "        # make a list from all the directories \n",
    "        subfolders = [os.path.join(r, folder) for folder in d]\n",
    "\n",
    "        for subf in subfolders:\n",
    "            # number of slices (images) in each DICOM folder, and the name of the folders\n",
    "            slice_num = len(glob.glob(subf+\"/*.DCM\"))\n",
    "\n",
    "            # find whether subf is a path and the number of .DCM images is more than 50\n",
    "            if slice_num > min_slice_num:\n",
    "\n",
    "                # Extract the information of the image \n",
    "                image=pdcm.dcmread(glob.glob(subf+\"/*.DCM\")[0],force=True)\n",
    "                folder_name = get_folder_name(image, subf)\n",
    "    \n",
    "                # Extract the CTs\n",
    "                if image.Modality == modality and all(keyword not in folder_name.lower() for keyword in exclusion_set):\n",
    "   \n",
    "                    patient_id = get_patient_id(image)\n",
    "\n",
    "                    # split the name of the folder into strings of information\n",
    "                    names_list = folder_name.split()\n",
    "\n",
    "                    # Initialize the following three patameters\n",
    "                    saver = None\n",
    "                    hd_fov = 0\n",
    "                    fraction = None\n",
    "\n",
    "                    for number, name in enumerate(names_list):\n",
    "                        saver = get_probable_weklyct_name(name, number, names_list, saver) \n",
    "                        hd_fov = get_hd_fov(name, hd_fov)\n",
    "                        fraction = get_fraction(name, fraction)\n",
    "\n",
    "                    # Find different information\n",
    "                    date_info, weekday, week_num = get_date_information(image)\n",
    "                    slice_thickness = get_slice_thickness(image)\n",
    "                    contrast = get_contrast(image)\n",
    "                    pixel_spacing = get_pixel_spacing(image)\n",
    "                    uid = get_ref_uid(image)\n",
    "\n",
    "                    # Add the information of this group to the total dataset\n",
    "                    group.append({\n",
    "                                'ID': patient_id, 'folder_name': folder_name, 'date': date_info,\n",
    "                                'week_day': weekday, 'week_num': week_num, 'info_header': saver,\n",
    "                                'fraction': fraction, 'HD_FoV': hd_fov, 'slice_thickness': slice_thickness,\n",
    "                                'num_slices': slice_num, 'pixel_spacing': pixel_spacing, 'contrast': contrast,\n",
    "                                'UID': uid, 'path': subf\n",
    "                                })\n",
    "    \n",
    "    # Make a datafrme from the main folder\n",
    "    df = pd.DataFrame(group)\n",
    "\n",
    "    # Save the dataframe\n",
    "    df.to_excel(os.path.join(output_path,file_name), index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1'\n",
    "output_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1'\n",
    "\n",
    "# Correct this one in the main code, this folder have a name tha follow the following structure: 'General_information_{folder_name}.xlsx'\n",
    "file_name = 'General_information_ART_DATA1.xlsx' \n",
    "df = navigate_folder(path_folder, output_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our knowledge about weeklyCTs, we know that they are only available after 2014, so we can just remove the patients before this specific time. Moreover, since this program just navigate all the folders, there may be some duplicated data in those folders, so I need to erase them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    \"\"\"\n",
    "    clean the dataset\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Slice the part of the dataset after the mentioned time.\n",
    "    time_limit = pd.Timestamp('2014-01-01') # CONFIG File\n",
    "    df_copy = df_copy[pd.to_datetime(df_copy.date) > time_limit]\n",
    "\n",
    "    # Drop the doplicated folders\n",
    "    df_copy = df_copy.drop_duplicates(subset=['ID', 'folder_name', 'date'],\n",
    "                                       keep='first', inplace=False, ignore_index=True)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, I will drop all the remained CTs that are not WeeklyCTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_firstday(df, date_list):\n",
    "    try:\n",
    "        first_day = df[df.date == date_list[1]].iloc[0].week_day\n",
    "    except:\n",
    "        first_day = None\n",
    "    \n",
    "    return first_day\n",
    "\n",
    "def find_matching_header(info_headers):\n",
    "    for header in info_headers:\n",
    "        try:\n",
    "            lowercase_header = header.lower()\n",
    "\n",
    "            if any(keyword in lowercase_header for keyword in ['rct', 'w']) and re.search(r'\\d', header):\n",
    "                return header\n",
    "\n",
    "            elif 'wk..' in lowercase_header and not re.search(r'\\d', header):\n",
    "                return header\n",
    "\n",
    "            elif re.search(r'rct.*[..]|rct.*[#]', lowercase_header) and not re.search(r'\\d', header):\n",
    "                return header\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An exception occurred: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_weeklycts_names(df, date_list):\n",
    "\n",
    "    header_list = list()\n",
    "\n",
    "    # Find the headers\n",
    "    for session in date_list[1:]:\n",
    "        info_headers = df[df.date == session].info_header.tolist()\n",
    "        header = find_matching_header(info_headers)\n",
    "\n",
    "        header_list.append(header)\n",
    "\n",
    "    # Ensure the header_list has 9 elements\n",
    "    header_list += [None] * (9 - len(header_list))\n",
    "\n",
    "    return header_list\n",
    "\n",
    "def get_accelerated_rt(patient_id, clinical_df):\n",
    "    try:\n",
    "        accelerated_rt = clinical_df[clinical_df.UMCG==int(patient_id)].Modality_adjusted.values[0]\n",
    "    \n",
    "    except:\n",
    "        accelerated_rt = 'Not Mentioned'\n",
    "    \n",
    "    return accelerated_rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weeklyct_folders(df, output_path):\n",
    "    \"\"\"\n",
    "    This function finds weeklyCTs and drops other types of CTs\n",
    "    \"\"\"\n",
    "    # Call clinical df to extract Accelerated program for each patient\n",
    "    clinical_df_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART Hooman/Xerostomia_dataset.xlsx' # CONFIG File\n",
    "    clinical_df = pd.read_excel(clinical_df_path)\n",
    "\n",
    "    group = list()\n",
    "\n",
    "    # Separate each ID dataframe\n",
    "    id_df = pd.DataFrame(df.groupby(['ID']))\n",
    "\n",
    "    for counter, id_num in enumerate(id_df[0]):\n",
    "\n",
    "        df = id_df[1][counter]\n",
    "\n",
    "        # Extract the parts suspected to contain weeklyCTs\n",
    "        df = df[(df['folder_name'].str.lower().str.contains('rct') & (df['date'] != df['date'].min())) \\\n",
    "                | ((df['date'] == df['date'].min()))]\n",
    "       \n",
    "        date_list = sorted(list(df.date.unique())) # Find the list of dates\n",
    "        rtstart = date_list[0] # Extract RTSTART  \n",
    "        first_day = get_firstday(df, date_list) # the week day of the first treatment\n",
    "\n",
    "        # Extract the weeklyCTs names and first day of the treatment\n",
    "        header_list= get_weeklycts_names(df, date_list)\n",
    "\n",
    "        # Extract other parameters\n",
    "        durations = date_list[1:]\n",
    "        weekly_ct_num = len(durations)       \n",
    "        durations += [None] * (9 - len(durations)) # Ensure it has 9 elements\n",
    "        Modality_adjusted = get_accelerated_rt(id_num, clinical_df)\n",
    "\n",
    "        group.append({'ID': int(id_num), 'Baseline': rtstart, 'Session1': durations[0],\n",
    "                        'Session2': durations[1], 'Session3': durations[2],'Session4': durations[3],\n",
    "                        'Session5': durations[4], 'Session6': durations[5],'Session7': durations[6],\n",
    "                        'Session8': durations[7],'Session9': durations[8], 'Fraction1': header_list[0],\n",
    "                        'Fraction2': header_list[1], 'Fraction3': header_list[2],'Fraction4': header_list[3],\n",
    "                        'Fraction5': header_list[4], 'Fraction6': header_list[5], 'Fraction7': header_list[6],\n",
    "                        'Fraction8': header_list[7],'Fraction9': header_list[8], 'First_day': first_day,\n",
    "                        'Number_of_CTs': df.shape[0], 'Number_of_weeklyCTs': weekly_ct_num, 'modality_adjusted':Modality_adjusted})\n",
    "        \n",
    "    # Make a datafrme from the main folder\n",
    "    df_final = pd.DataFrame(group)\n",
    "\n",
    "    # Drop the patients who does not have weeklyCTs\n",
    "    df_final = df_final[~(df_final.Number_of_weeklyCTs == 0)]\n",
    "    df_final = df_final.reset_index().drop(columns=['index'])\n",
    "\n",
    "    # Save the dataframe\n",
    "    # df_final.to_csv(os.path.join(output_path, file_name), index=False)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly_file_name = 'weeklyct_output.csv' DO NOT NEED THIS ONE, IT IS A MIDDLE PROCESS\n",
    "weeklyct_df = extract_weeklyct_folders(df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of Navigation Phase, I will replace all the strings in the header part of the dataframe into fraction numbers. There are multiple conditions here. some patients have their own fractions in their headers e.g. 'rct13', but some others have week number like 'wk3' or have a part of the repeated CT name such as 'rct..', 'wk', 'wk..', and so on. for the first group, I just use the number of fractions in the header. However, for the second and third group, I calculate the probable numeber of fractions using the following criteria.\n",
    "if the patient has accelarated RT plan, I assume that they should get 1.2 fraction per day (only in working days), so it mean 6 fractions per week.Ans, for patients with other types of the treatment, I suppose that they  should get 1 fraction per working day, so in total 5 per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to extract numbers only if 'wk' is not present\n",
    "def extract_numbers(text):\n",
    "    if isinstance(text, str) and 'wk' not in text and re.search(r'\\d', text):\n",
    "        \n",
    "        return  float(''.join(filter(str.isdigit, text)))       \n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def get_existing_fractions(df):\n",
    "    \"\"\"\n",
    "    This function extract all the fractions exist in the data itself.\n",
    "    \"\"\"\n",
    "    for header in df.iloc[:, 11:20].columns:\n",
    "        df[header] = df[header].apply(extract_numbers)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_coef(Modality_adjusted):\n",
    "    \"\"\"\n",
    "    Get the coefficient of the fractions\n",
    "    \"\"\"\n",
    "    accelerated_list = ['Accelerated RT', 'Bioradiation'] # CONFIG File\n",
    "    not_accelerated_list = ['Chemoradiation', 'Conventional RT'] # CONFIG File\n",
    "    \n",
    "    if Modality_adjusted in not_accelerated_list:\n",
    "        coef = 1.0\n",
    "    \n",
    "    elif Modality_adjusted in accelerated_list:\n",
    "        coef = 1.2\n",
    "\n",
    "    else:\n",
    "        coef = 0.0\n",
    "\n",
    "    return coef\n",
    "\n",
    "def calculate_fraction(raw, fraction, fraction_num, coef, counter):\n",
    "    try:\n",
    "    \n",
    "        if isinstance(fraction, str) and 'wk' in fraction and  counter == 0:\n",
    "            fraction_num = (len(pd.bdate_range( raw[f'Baseline'], raw[f'Session{1}'])) - 1) * coef + 1\n",
    "\n",
    "        elif isinstance(fraction, str) and 'wk' in fraction and  counter != 0:\n",
    "            fraction_num += (len(pd.bdate_range( raw[f'Session{counter}'], raw[f'Session{counter+1}'])) - 1) * coef\n",
    "                \n",
    "        elif isinstance(fraction, str) and 'wk' not in fraction and not re.search(r'\\d', fraction) and counter==0:\n",
    "            fraction_num += (len(pd.bdate_range( raw[f'Baseline'], raw[f'Session{1}'])) - 1) * coef + 1\n",
    "\n",
    "        # This part does not work  if the rct.. or rct# is seperated from other part\n",
    "        elif isinstance(fraction, str) and 'wk' not in fraction and not re.search(r'\\d', fraction) and counter!=0:\n",
    "            fraction_num += (len(pd.bdate_range( raw[f'Session{counter}'], raw[f'Session{counter+1}'])) - 1) * coef\n",
    "\n",
    "        elif fraction is np.nan and counter < raw.Number_of_weeklyCTs and counter==0:\n",
    "            fraction_num = (len(pd.bdate_range( raw[f'Baseline'], raw[f'Session{1}'])) - 1) * coef + 1\n",
    "\n",
    "        elif fraction is np.nan and counter < raw.Number_of_weeklyCTs and counter!=0:\n",
    "            fraction_num += (len(pd.bdate_range( raw[f'Session{counter}'], raw[f'Session{counter+1}'])) - 1) * coef              \n",
    "\n",
    "        elif isinstance(fraction, int) or isinstance(fraction, float):\n",
    "            fraction_num = fraction\n",
    "\n",
    "        else:\n",
    "            fraction_num = None\n",
    "        return fraction_num \n",
    "\n",
    "    except:\n",
    "        return fraction_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fractions(df, output_path, file_name):\n",
    "    \"\"\"\n",
    "    This function finds or calculates all the fractions\n",
    "    \"\"\"\n",
    "    # Make a copy of the dataset\n",
    "    df_copy = df.copy()\n",
    "    coef_list = list()\n",
    "    # Find all the existing fractions in the dataset\n",
    "    df_copy = get_existing_fractions(df_copy)\n",
    "\n",
    "    # Iterate through patients\n",
    "    for index, raw in df_copy.iterrows():\n",
    "\n",
    "        fraction_list = list()\n",
    "        fraction_num = 0\n",
    "\n",
    "        # Calculate the coefficient\n",
    "        coef = get_coef(raw.modality_adjusted)\n",
    "\n",
    "        # Iterate through fractions\n",
    "        for counter, fraction in enumerate(raw.iloc[11:20]):\n",
    "\n",
    "            # Calculate and add different fractions to the list of fractions\n",
    "            fraction_num = calculate_fraction(raw, fraction, fraction_num, coef, counter)\n",
    "            fraction_list.append(fraction_num)\n",
    "\n",
    "        df_copy.iloc[index, 11:20] = fraction_list\n",
    "        coef_list.append(coef)\n",
    " \n",
    "    df_copy['Coefficient'] = coef_list\n",
    "\n",
    "    # Save the dataframe\n",
    "    df_copy.to_excel(os.path.join(output_path, file_name), index=False)\n",
    "\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct this one in the main code, this folder have a name tha follow the following structure: 'WeeklyCTs_fraction_{folder_name}.xlsx'\n",
    "file_name = 'WeeklyCTs_fraction_ART_DATA1.xlsx'\n",
    "weeklyct_df = add_fractions(weeklyct_df, output_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the first phase can be extracting the information of a specific week e.g. week3. To achieve this aim, I will make a function, that can be call and return an excel file for patients who have a specific week fraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_week_information(main_df, weeklyct_df, week_name):\n",
    "\n",
    "    accelerated_list = ['Accelerated RT', 'Bioradiation'] # CONFIG File\n",
    "    not_accelerated_list = ['Chemoradiation', 'Conventional RT'] # CONFIG File\n",
    "    fraction_range_dict = {'week1': {'not_accelerated':[0.0, 5.0], 'accelerated': [0.0, 6.0]}, # Config File\n",
    "                           'week2': {'not_accelerated':[5.0, 10.0], 'accelerated': [6.0, 12.0]},\n",
    "                           'week3': {'not_accelerated':[10.0, 15.0], 'accelerated': [12.0, 18.0]},\n",
    "                           'week4': {'not_accelerated':[15.0, 20.0], 'accelerated': [18.0, 24.0]},\n",
    "                           'week5': {'not_accelerated':[20.0, 25.0], 'accelerated': [24.0, 30.0]},\n",
    "                           'week6': {'not_accelerated':[25.0, 30.0], 'accelerated': [30.0, 36.0]},\n",
    "                           'week7': {'not_accelerated':[30.0, 35.0], 'accelerated': [36.0, 42.0]},\n",
    "                           'week8': {'not_accelerated':[35.0, 40.0], 'accelerated': [42.0, 48.0]}}\n",
    "    week_list = list()\n",
    "\n",
    "    # Iterate through patients\n",
    "    for _, raw in weeklyct_df.iterrows():\n",
    "        matching_list = []\n",
    "        fraction_seri = raw.iloc[11:20]\n",
    "        #print(raw.modality_adjusted)\n",
    "        # Find any columns that have values inside the range of a a specific week\n",
    "        if raw.modality_adjusted in not_accelerated_list:\n",
    "            matching_list = [column for column in fraction_seri.index \\\n",
    "            if (raw[column]is not None and raw[column] > fraction_range_dict[week_name]['not_accelerated'][0] \\\n",
    "                and raw[column] <= fraction_range_dict[week_name]['not_accelerated'][1])]\n",
    "\n",
    "        elif raw.modality_adjusted in accelerated_list:\n",
    "            matching_list = [column for column in fraction_seri.index \\\n",
    "            if (raw[column]is not None and raw[column] > fraction_range_dict[week_name]['accelerated'][0] \\\n",
    "                and raw[column] <= fraction_range_dict[week_name]['accelerated'][1])]\n",
    "        # print(matching_list)\n",
    "        # If finds a column, add some information of  that patient to the dictionary\n",
    "        if len(matching_list) > 0:\n",
    "            for matched_fraction in matching_list:\n",
    "                week_num = matched_fraction[-1]\n",
    "                week_list.append({'ID': raw.ID,\n",
    "                                 'date': raw[f'Session{week_num}'],\n",
    "                                 'treatment_week': week_name,\n",
    "                                 'Fraction_num': matched_fraction, \n",
    "                                 'Fraction_magnitude': raw[matched_fraction], \n",
    "                                 'modality_adjusted': raw.modality_adjusted})\n",
    "            # print(week_list)\n",
    "    # Make a datafrme from the main folder\n",
    "    week_df = pd.DataFrame(week_list)\n",
    "    \n",
    "    try:\n",
    "        final_df = week_df.merge(main_df, on=['ID', 'date']).drop(columns=['fraction'])\n",
    "    \n",
    "    except KeyError:\n",
    "        print(f'Warning: this week dataset has {week_df.shape} shape')\n",
    "        final_df = pd.DataFrame()\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_df = get_a_week_information(df, weeklyct_df, 'week6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Phase\n",
    "In this phase, dataframes from different folders (it can be one or more folders) gather to gether to make a total dataframe for all the dataset in different folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(name):\n",
    "\n",
    "    output_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1' # CONFIG File\n",
    "\n",
    "    try:\n",
    "        # If the file is an excel file\n",
    "        if '.xlsx' in name:\n",
    "            df = pd.read_excel(os.path.join(output_path, name))\n",
    "            \n",
    "        # If the file is a csv file\n",
    "        elif '.csv' in name:\n",
    "            df = pd.read_csv(os.path.join(output_path, name)) # Comma seperated\n",
    "\n",
    "            # If the csv file is semi-colon seperated\n",
    "            if ';' in df.columns[0]:\n",
    "                df = pd.read_csv(os.path.join(output_path, name), sep=';')\n",
    "\n",
    "        # Erase the index columns if there is any\n",
    "        if any('unnamed' in col_name.lower() for col_name in df.columns):\n",
    "            excess_column_names = [col_name for col_name in df.columns if 'unnamed' in col_name.lower()]\n",
    "            df = df.drop(columns=excess_column_names)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'Warning: file {name} was not found')\n",
    "    \n",
    "    except ValueError:\n",
    "        print(f'File {name} is not supported by this program.')\n",
    "\n",
    "\n",
    "def concat_dataframes(df_name_list):\n",
    "    \"\"\"\n",
    "    This function accepts excel and csv files. csvs can be comma-seperated or semicolon-seperated\n",
    "    \"\"\"\n",
    "    # Make an empty df to gather all of the dataframes here.\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for name in df_name_list:\n",
    "        df = read_dataframe(name)\n",
    "\n",
    "        try:\n",
    "           final_df = pd.concat([final_df, df], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'ERROR:error {e} ocurs for {name} folder')\n",
    "            pass\n",
    "\n",
    "    # Drop duplicated patients\n",
    "    if 'weeklyct' in df_name_list[0].lower(): \n",
    "        final_df = final_df.drop_duplicates(subset=['ID'])\n",
    "\n",
    "    # Reset the index\n",
    "    final_df = final_df.sort_values('ID').reset_index().drop(columns=['index'])\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, I will use the above functions to make two final datasets: WeeklyCT_dataset and General_dataset.\n",
    "\n",
    "### WeeklyCT Final Dataframe\n",
    "This dataset contains clinical and some technical information about the patients who have WeeklyCTs. This dataset will be used further in plotting phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataframes(desired_file):\n",
    "    \"\"\"\n",
    "    This function makes the list of the desired file names. It can be weeklyCT files or General files\n",
    "    \"\"\"\n",
    "    output_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1' # CONFIG File\n",
    "\n",
    "    # Find all the relavant dataframes\n",
    "    file_list = os.listdir(output_path)\n",
    "    desired_file_list = [file_name for file_name in file_list if desired_file in file_name.lower()]\n",
    "\n",
    "    return desired_file_list\n",
    "\n",
    "def call_clinical_dataframe():\n",
    "    clinical_df_name = 'Xerostomia_dataset.xlsx' # CONFIG File\n",
    "\n",
    "    # Define a mapping between source and target column names\n",
    "    column_mapping = {'UMCG': 'ID', # CONFIG File\n",
    "                      'GESLACHT': 'gender', \n",
    "                      'LEEFTIJD': 'age',\n",
    "                      'Loctum2': 'tumor_location',\n",
    "                      'N_stage': 'n_stage',\n",
    "                      'TSTAD_DEF': 't_stage',\n",
    "                      'HN35_Xerostomia_M06': 'xer_06',\n",
    "                      'HN35_Xerostomia_M12': 'xer_12'}   \n",
    "    \n",
    "    clinical_df = read_dataframe(clinical_df_name)\n",
    "    desired_column_list = list(column_mapping.keys())\n",
    "\n",
    "    # Slice the desired part\n",
    "    clinical_df = clinical_df.loc[:,desired_column_list]\n",
    "\n",
    "    # Map the name of the columns to the desired names\n",
    "    clinical_df = clinical_df.rename(columns=column_mapping)\n",
    "\n",
    "    return clinical_df\n",
    "\n",
    "\n",
    "def make_weeklyct_dataframe():\n",
    "    \"\"\"\n",
    "    This function makes the final weeklyCT dataframe\n",
    "    \"\"\"\n",
    "    make_label_df = True # Config File\n",
    "    label_list = ['xer_06', 'xer_12'] # Config File\n",
    "\n",
    "    file_names = find_dataframes('weeklyct')\n",
    "    df = concat_dataframes(file_names)\n",
    "    clinical_df = call_clinical_dataframe()\n",
    "    final_weeklyct_df = df.merge(clinical_df, on='ID')\n",
    "\n",
    "    # Save the dataframe\n",
    "    final_weeklyct_df.to_excel(os.path.join(output_path, 'Overview_weeklyCT_patients.xlsx'), index=False)\n",
    "\n",
    "    # If dataframe based on labels is needed\n",
    "    if make_label_df:\n",
    "        for label in label_list:\n",
    "            label_df = final_weeklyct_df[final_weeklyct_df[label].notnull()]\n",
    "            label_df.to_excel(os.path.join(output_path, f'Overview_weeklyCT_patients_{label}.xlsx'), index=False)\n",
    "\n",
    "    return final_weeklyct_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df = make_weeklyct_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Dataframe\n",
    "This dataframe contains the information of the available weeklyCT folder. This dataframe will be used further in transferring phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_general_dataframe():\n",
    "    week_list = ['week1', 'week2', 'week3', 'week4', 'week5', 'week6', 'week7', 'week8'] # CONFIG File (It can be week dictionary key list)\n",
    "    output_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1' # CONFIG File\n",
    "\n",
    "    # Make a dataframe from all the general files\n",
    "    file_names = find_dataframes('general')\n",
    "    general_df = concat_dataframes(file_names)\n",
    "    weekly_df = pd.read_excel(os.path.join(output_path, 'Overview_weeklyCT_patients.xlsx'))\n",
    "\n",
    "    final_general_df = pd.DataFrame()\n",
    "\n",
    "    # Make the datframe for each week and concat all of them to make a dataset\n",
    "    for week_name in week_list:\n",
    "        week_df = get_a_week_information(general_df, weekly_df, week_name)\n",
    "        final_general_df = pd.concat([final_general_df, week_df], ignore_index=True)\n",
    "    \n",
    "    # Sort the dataset based on ID\n",
    "    final_general_df = final_general_df.sort_values('ID').reset_index().drop(columns=['index'])\n",
    "    # Save the dataframe\n",
    "    final_general_df.to_excel(os.path.join(output_path, 'General_information.xlsx'), index=False)\n",
    "\n",
    "    return final_general_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: this week dataset has (0, 0) shape\n",
      "Warning: this week dataset has (0, 0) shape\n",
      "Warning: this week dataset has (0, 0) shape\n"
     ]
    }
   ],
   "source": [
    "final_general_df = make_general_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>treatment_week</th>\n",
       "      <th>Fraction_num</th>\n",
       "      <th>Fraction_magnitude</th>\n",
       "      <th>modality_adjusted</th>\n",
       "      <th>folder_name</th>\n",
       "      <th>week_day</th>\n",
       "      <th>week_num</th>\n",
       "      <th>info_header</th>\n",
       "      <th>HD_FoV</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>num_slices</th>\n",
       "      <th>pixel_spacing</th>\n",
       "      <th>contrast</th>\n",
       "      <th>UID</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20715</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>week1</td>\n",
       "      <td>Fraction1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chemoradiation</td>\n",
       "      <td>rCT3 protonen  2.0  HD_FoV imar   iMAR</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>rCT3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>206</td>\n",
       "      <td>[1.0546875, 1.0546875]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.12.2.1107.5.1.4.95434.30000018041806221769...</td>\n",
       "      <td>//zkh/appdata/RTDicom/Projectline_HNC_modellin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20715</td>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>week3</td>\n",
       "      <td>Fraction2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Chemoradiation</td>\n",
       "      <td>rCT13 protonen  2.0  I40s  3 imar   iMAR</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>rCT13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>206</td>\n",
       "      <td>[0.9765625, 0.9765625]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.12.2.1107.5.1.4.95434.30000018050206052379...</td>\n",
       "      <td>//zkh/appdata/RTDicom/Projectline_HNC_modellin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20715</td>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>week3</td>\n",
       "      <td>Fraction2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Chemoradiation</td>\n",
       "      <td>w3CT_reg_bsl</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>w3CT_reg_bsl</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>206</td>\n",
       "      <td>[0.9765625, 0.9765625]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.12.2.1107.5.1.4.95434.30000018032706004286...</td>\n",
       "      <td>//zkh/appdata/RTDicom/Projectline_HNC_modellin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20715</td>\n",
       "      <td>2018-05-09</td>\n",
       "      <td>week4</td>\n",
       "      <td>Fraction3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Chemoradiation</td>\n",
       "      <td>rCT18 protonen  2.0  I40s  3 imar   iMAR</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>rCT18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>206</td>\n",
       "      <td>[0.9765625, 0.9765625]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.12.2.1107.5.1.4.95434.30000018050905575678...</td>\n",
       "      <td>//zkh/appdata/RTDicom/Projectline_HNC_modellin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>303865</td>\n",
       "      <td>2015-12-23</td>\n",
       "      <td>week5</td>\n",
       "      <td>Fraction1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Chemoradiation</td>\n",
       "      <td>HerCTHH wk 3  2.0  I40s  3</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>wk3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>195</td>\n",
       "      <td>[0.9765625, 0.9765625]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.12.2.1107.5.1.4.95434.30000015122307471636...</td>\n",
       "      <td>//zkh/appdata/RTDicom/Projectline_HNC_modellin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID       date treatment_week Fraction_num  Fraction_magnitude  \\\n",
       "0   20715 2018-04-18          week1    Fraction1                 3.0   \n",
       "1   20715 2018-05-02          week3    Fraction2                13.0   \n",
       "2   20715 2018-05-02          week3    Fraction2                13.0   \n",
       "3   20715 2018-05-09          week4    Fraction3                18.0   \n",
       "4  303865 2015-12-23          week5    Fraction1                25.0   \n",
       "\n",
       "  modality_adjusted                               folder_name  week_day  \\\n",
       "0    Chemoradiation    rCT3 protonen  2.0  HD_FoV imar   iMAR         3   \n",
       "1    Chemoradiation  rCT13 protonen  2.0  I40s  3 imar   iMAR         3   \n",
       "2    Chemoradiation                              w3CT_reg_bsl         3   \n",
       "3    Chemoradiation  rCT18 protonen  2.0  I40s  3 imar   iMAR         3   \n",
       "4    Chemoradiation                HerCTHH wk 3  2.0  I40s  3         3   \n",
       "\n",
       "   week_num   info_header  HD_FoV  slice_thickness  num_slices  \\\n",
       "0        16          rCT3       1                2         206   \n",
       "1        18         rCT13       0                2         206   \n",
       "2        18  w3CT_reg_bsl       0                2         206   \n",
       "3        19         rCT18       0                2         206   \n",
       "4        52           wk3       0                2         195   \n",
       "\n",
       "            pixel_spacing  contrast  \\\n",
       "0  [1.0546875, 1.0546875]         0   \n",
       "1  [0.9765625, 0.9765625]         0   \n",
       "2  [0.9765625, 0.9765625]         0   \n",
       "3  [0.9765625, 0.9765625]         0   \n",
       "4  [0.9765625, 0.9765625]         0   \n",
       "\n",
       "                                                 UID  \\\n",
       "0  1.3.12.2.1107.5.1.4.95434.30000018041806221769...   \n",
       "1  1.3.12.2.1107.5.1.4.95434.30000018050206052379...   \n",
       "2  1.3.12.2.1107.5.1.4.95434.30000018032706004286...   \n",
       "3  1.3.12.2.1107.5.1.4.95434.30000018050905575678...   \n",
       "4  1.3.12.2.1107.5.1.4.95434.30000015122307471636...   \n",
       "\n",
       "                                                path  \n",
       "0  //zkh/appdata/RTDicom/Projectline_HNC_modellin...  \n",
       "1  //zkh/appdata/RTDicom/Projectline_HNC_modellin...  \n",
       "2  //zkh/appdata/RTDicom/Projectline_HNC_modellin...  \n",
       "3  //zkh/appdata/RTDicom/Projectline_HNC_modellin...  \n",
       "4  //zkh/appdata/RTDicom/Projectline_HNC_modellin...  "
      ]
     },
     "execution_count": 1264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_general_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferring Phase\n",
    "In this phase, all the new weeklyCTs will be transferred into the determined and final folder. If there is the same weeklyCT with the same Patient ID, fraction and week number, this program skips that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfering_weeklycts(directory_df):\n",
    "\n",
    "    destination_main = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1' # CONFIG File\n",
    "\n",
    "    # Keep track of the patients\n",
    "    previous_patient_id = None\n",
    "\n",
    "    # For each CT scan, iterate through the information\n",
    "    for index, row in directory_df.iterrows():\n",
    "        current_patient_id = row.ID\n",
    "\n",
    "        if current_patient_id != previous_patient_id:\n",
    "            print(f'Transferring data for patient {current_patient_id} is started')\n",
    "\n",
    "        # List the direction to the DICOM files\n",
    "        dicom_files = os.listdir(row.path)\n",
    "\n",
    "        # Make the destination directory\n",
    "        final_destination_path = os.path.join(destination_main, str(row.ID), str(f'{row.treatment_week}_{row.Fraction_magnitude}'))\n",
    "\n",
    "        # Try to make the destination directory\n",
    "        try:\n",
    "            os.makedirs(final_destination_path, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "\n",
    "        # Loop through all the CT images\n",
    "        for file in dicom_files:\n",
    "            src_path = os.path.join(row.path, file)\n",
    "            dst_path = os.path.join(final_destination_path, file)\n",
    "\n",
    "            # Transfer the data to the destination directory\n",
    "            try:\n",
    "                shutil.copy(src_path, dst_path)  # Use shutil.copy to copy the file\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying file: {e}\")\n",
    "    \n",
    "        if current_patient_id != previous_patient_id :\n",
    "            previous_patient_id  = current_patient_id\n",
    "            print(f'Transferring data for patient {current_patient_id} is ended {index}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring data for patient 20715 is started\n",
      "Transferring data for patient 20715 is ended 0\n",
      "Transferring data for patient 303865 is started\n",
      "Transferring data for patient 303865 is ended 4\n",
      "Transferring data for patient 1320543 is started\n",
      "Transferring data for patient 1320543 is ended 7\n"
     ]
    }
   ],
   "source": [
    "transfering_weeklycts(final_general_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Phase\n",
    "This phase is the last phase in this analyzing notebook. The watching dog program will be added to the main program. \n",
    "\n",
    "To present a summary and also some information about my dataset, I decided to make a representative panel that can be used to transfer much information to the user. In this panel. I used the base structure designed with one of my friends, and then I added the plots and datasets to it. I made this plotting class based on two endpoints (xerostomia 6-month and zerostomia 12-month); however, it is adjustable for other endponts as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarChart:\n",
    "    def __init__(self, df, columns_names, param):\n",
    "        self.df = df\n",
    "        self.columns_names = columns_names\n",
    "        self.param = param\n",
    "    \n",
    "    def bar_chart_panel_maker(self):\n",
    "        \"\"\"\n",
    "        Type: Instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This method assembel all the elements of the pannel and also uses \n",
    "                     _update_choices to update the choices of each parameter.\n",
    "\n",
    "        Output: 1. interactive bar plot pannel\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the widgets\n",
    "        select_param, multi_choice_param = self.widgit_maker()\n",
    "\n",
    "        # Update the choices\n",
    "        @pn.depends(select_param[1].param.value, watch=True)\n",
    "        def _update_choices(select_param):\n",
    "            \"\"\"\n",
    "            Type: dependent function\n",
    "\n",
    "            Input: 1. the parameter selected by a user\n",
    "\n",
    "            Explanation: This function is responsible for updating the choices for each parameter.\n",
    "\n",
    "            Output: ---\n",
    "            \"\"\"\n",
    "            self.param = select_param\n",
    "            # Extract the new dataset for sketching\n",
    "            sketch_series = self.data_maker()\n",
    "\n",
    "            # Change the options and values of the multi_choice widgit for each parameter.\n",
    "            multi_choice_param[1].options = list(sketch_series.index)\n",
    "            multi_choice_param[1].value = list(sketch_series.index)\n",
    "\n",
    "        # Bind all the elements of the interactive plot\n",
    "        inter_plot = pn.bind(self.render_plot, parameter=select_param[1], choices=multi_choice_param[1])\n",
    "\n",
    "        # encapsulating the iter_plot to link the plot with the widgits\n",
    "        final_plot = pn.Row(pn.Column(select_param, multi_choice_param), pn.pane.Bokeh(inter_plot)) \n",
    "\n",
    "        return final_plot\n",
    "    \n",
    "    def widgit_maker(self):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This method makes selection bar and multichoice box.\n",
    "\n",
    "        Output: 1. Selection bar.\n",
    "                2. multi-choice box.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the initial dataset\n",
    "        multi_initial_items = self.data_maker()\n",
    "\n",
    "        # Make the select bar and assign a name to it\n",
    "        param_name = pn.pane.Markdown(f'**Parameters**')\n",
    "        select_param = pn.widgets.Select(value=self.param, options=self.columns_names)\n",
    "        select_param_layout = pn.Column(param_name, select_param)\n",
    "\n",
    "        # Make the multi-choice box and assign a name to it \n",
    "        item_name = pn.pane.Markdown(f'**Items**')\n",
    "        multi_choice_param = pn.widgets.MultiChoice( value = list(multi_initial_items.index),\n",
    "                                                    options= list(multi_initial_items.index))\n",
    "        choice_item_layout = pn.Column(item_name, multi_choice_param)\n",
    "\n",
    "        return select_param_layout, choice_item_layout\n",
    "\n",
    "    \n",
    "    def render_plot(self, parameter, choices):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: 1. parameter: is the parameter like 'year' that we want to sketch its bar plot.\n",
    "               2. choices: is the group of available items for each parameter.\n",
    "\n",
    "        Explanation: This metehod sketch the bar plot based on the parameter of interest and return it.\n",
    "\n",
    "        Output: 1. bar plot.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the dataset\n",
    "        sketch_series = self.data_maker()\n",
    "\n",
    "        # Extract the dataset for x and y axis\n",
    "        xaxis = [item for item in sketch_series.index if item in choices]\n",
    "        yaxis = [sketch_series[value] for value in xaxis]\n",
    "\n",
    "        # Make  the dataset in ColumnDataSource format\n",
    "        source = ColumnDataSource(dict(x = xaxis, y = yaxis))\n",
    "\n",
    "        # Design the plot infrestructure \n",
    "        fig = self.fig_maker(parameter, source, xaxis)\n",
    "\n",
    "        # Add bar plots to the main plot\n",
    "        fig.vbar(x=xaxis, top=yaxis, width=0.8, color='#EE8262')\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def fig_maker(self, parameter, source, xaxis):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: 1. parameter: is the parameter like 'year' that we want to sketch its bar plot.\n",
    "               2. choices: is the group of available items for each parameter.\n",
    "\n",
    "        Explanation: This metehod sketch the bar plot based on the parameter of interest and return it.\n",
    "\n",
    "        Output: 1. bar plot.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make labels\n",
    "        y_label = 'number of patients'\n",
    "        title = f'number of patients per {parameter}'\n",
    "\n",
    "        # If the dataset has a list of string as the xaxis make the following figure\n",
    "        try:\n",
    "            fig = figure(width=800, height=600,        \n",
    "                            x_axis_label = parameter,\n",
    "                            y_axis_label = y_label,\n",
    "                            title=title,\n",
    "                            x_range=xaxis)\n",
    "\n",
    "        # Otherwise, make the following figure\n",
    "        except:\n",
    "            fig = figure(width=800, height=600,        \n",
    "                            x_axis_label = parameter,\n",
    "                            y_axis_label = y_label,\n",
    "                            title=title)\n",
    "\n",
    "        # Add labels to the main figure\n",
    "        labels = LabelSet(x='x', y='y', text='y', level='glyph',\n",
    "                        text_align='center', y_offset=5, source=source)\n",
    "        \n",
    "        fig.title.align = 'center'\n",
    "        fig.add_layout(labels)\n",
    "        return fig\n",
    "\n",
    "    def data_maker(self):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This metehod makes the dataset that should be sketch as a barplot.\n",
    "\n",
    "        Output: 1. figure dataset.\n",
    "        \"\"\"\n",
    "        if self.param == 'Year':\n",
    "            sketch_series = self.df.RTSTART.dt.year.value_counts()\n",
    "\n",
    "        elif self.param == 'Count_of_weeks':\n",
    "            sketch_series = self.week_count_maker().sort_index()\n",
    "\n",
    "        elif self.param == 'age':\n",
    "            sketch_series = self.age_count_maker().sort_index()\n",
    "\n",
    "        elif self.param == 'First_day':\n",
    "            sketch_series = self.fday_count_maker()\n",
    "\n",
    "        elif self.param == 'n_stage' or self.param == 't_stage':\n",
    "            sketch_series = self.stage_count_maker().sort_index()\n",
    "\n",
    "        elif self.param == 'xer_06' or self.param == 'xer_12':\n",
    "            sketch_series = self.xer_count_maker().sort_index()  \n",
    "\n",
    "        elif self.param == 'xer_trend':\n",
    "            sketch_series = self.trend_count_maker().sort_index()   \n",
    "\n",
    "        else:\n",
    "            sketch_series = self.df[self.param].value_counts()\n",
    "        \n",
    "        return sketch_series\n",
    "\n",
    "    def trend_count_maker(self):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This metehod counts the number of different trend for patient who are\n",
    "                     diagnosed with xerostomia.\n",
    "\n",
    "        Output: 1. a series that contains number of patients per different trends.\n",
    "        \"\"\"        \n",
    "        trend_df = self.df[['xer_06', 'xer_12']]\n",
    "        stage_list = [self.map_element_to_trend(element) for _, element in trend_df.iterrows()]\n",
    "        stage_df = pd.DataFrame(stage_list).transpose()\n",
    "        return stage_df.stack().value_counts()\n",
    "\n",
    "    def xer_count_maker(self):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This metehod counts the number of labels for xerostomia endpoints.\n",
    "\n",
    "        Output: 1. a series that contains number of patients per label.\n",
    "        \"\"\"\n",
    "        xer_series = self.df[self.param]\n",
    "        xer_list = [self.map_element_to_xer(element) for element in xer_series]\n",
    "        xer_df = pd.DataFrame(xer_list).transpose()\n",
    "        return xer_df.stack().value_counts()        \n",
    "\n",
    "    def stage_count_maker(self):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This metehod counts the number of patients in different t or n stages.\n",
    "\n",
    "        Output: 1. a series that contains number of patients per different t or n stages.\n",
    "        \"\"\"\n",
    "        stage_series = self.df[self.param]\n",
    "        stage_list = [self.map_element_to_stage(element, self.param) for element in stage_series]\n",
    "        stage_df = pd.DataFrame(stage_list).transpose()\n",
    "        return stage_df.stack().value_counts()\n",
    "\n",
    "    def fday_count_maker(self):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This metehod counts the number of patients per the day of the week that day start\n",
    "                     their treatment.\n",
    "\n",
    "        Output: 1. a series that contains number of patients per day of the week.\n",
    "        \"\"\"\n",
    "        fday_series = self.df.First_day\n",
    "        fday_list = [self.map_element_to_fday(element) for element in fday_series]\n",
    "        fday_df = pd.DataFrame(fday_list).transpose()\n",
    "        fday_df = fday_df.stack().value_counts()\n",
    "        \n",
    "        # Define the desired order of days of the week\n",
    "        desired_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "\n",
    "        # Reindex the series to match the desired order\n",
    "        ordered_series = fday_df.reindex(desired_order)    \n",
    "        return ordered_series\n",
    "\n",
    "    def age_count_maker(self):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This metehod counts the number of patients per age group.\n",
    "\n",
    "        Output: 1. a series that contains number of patients per age group.\n",
    "        \"\"\"\n",
    "        age_series = self.df.age\n",
    "\n",
    "        age_list = [self.map_element_to_age(element) for element in age_series]\n",
    "\n",
    "        age_df = pd.DataFrame(age_list).transpose()\n",
    "        return age_df.stack().value_counts()\n",
    "\n",
    "\n",
    "    def week_count_maker(self):\n",
    "        \"\"\"\n",
    "        Type: instance method\n",
    "\n",
    "        Input: ---\n",
    "\n",
    "        Explanation: This metehod counts the number of weekly CTs for each week and make a series of number\n",
    "                     of patients per each week with number of patients as values and week number as the index.\n",
    "\n",
    "        Output: 1. a series that contains number of patients per week number.\n",
    "        \"\"\"\n",
    "        fraction_df = self.df.loc[:,'Fraction1':'Fraction9']\n",
    "        fraction_df['accelerated_rt'] = self.df['accelerated_rt']\n",
    "        week_list = [fraction_df.apply(lambda row: self.map_element_to_week(row[counter], row[-1]), axis=1) \n",
    "                    for counter in range(fraction_df.shape[1] - 1)]\n",
    "\n",
    "        week_df = pd.DataFrame(week_list).transpose()\n",
    "        return week_df.stack().value_counts()\n",
    "    \n",
    "    @staticmethod\n",
    "    def map_element_to_week(element, accelerated_rt):\n",
    "        \"\"\"\n",
    "        Type: static method\n",
    "\n",
    "        Input: 1. element: Is the element that we want to find a week for it.\n",
    "               2. accelerated_rt: is the plan type of the patient.\n",
    "\n",
    "        Explanation: this static method get an element let's say 7 with a RT plan type, then assign a proper\n",
    "                     week for this element and return the week.\n",
    "\n",
    "        Output: 1. is a week.\n",
    "        \"\"\"\n",
    "        if accelerated_rt == 0:\n",
    "            element_ranges = {(0, 5): 'Week1', (5, 10): 'Week2', (10, 15): 'Week3', (15, 20): 'Week4', \n",
    "                              (20, 25): 'Week5', (25, 30): 'Week6', (30, 35): 'Week7'}\n",
    "\n",
    "        else:\n",
    "            element_ranges = {(0, 6): 'Week1', (6, 12): 'Week2', (12, 18): 'Week3',\n",
    "                           (18, 24): 'Week4', (24, 30): 'Week5', (30, 36): 'Week6'}\n",
    "        \n",
    "        for key in element_ranges.keys():\n",
    "\n",
    "            if element > key[0] and element <= key[1]:\n",
    "                return element_ranges[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def map_element_to_age(individual_age):\n",
    "        \"\"\"\n",
    "        Type: static method\n",
    "\n",
    "        Input: 1. individual_age: Is the element that we want to find a an age category for .\n",
    "\n",
    "        Explanation: this static method get an element and find a proper age category for it.\n",
    "\n",
    "        Output: 1. is a age category.\n",
    "        \"\"\"\n",
    "        age_conditions_dict = {(0, 18):'Under 18', (18, 29): '20-29', (29, 39): '30-39',\n",
    "                               (39, 49): '40-49', (49, 59): '50-59', (59, 69): '60-69',\n",
    "                               (69, 79): '70-79', (79, 89): '80-89', (89, 99): '90-99'}\n",
    "\n",
    "        for key in age_conditions_dict.keys():\n",
    "\n",
    "            if individual_age > key[0] and individual_age <= key[1]:\n",
    "                return age_conditions_dict[key]\n",
    "    \n",
    "    @staticmethod\n",
    "    def map_element_to_fday(individual_day):\n",
    "        \"\"\"\n",
    "        Type: static method\n",
    "\n",
    "        Input: 1. individual_day: Is the element that we want to find a day in week for.\n",
    "\n",
    "        Explanation: this static method get an element and find a proper day in the week.\n",
    "\n",
    "        Output: 1. is a day name.\n",
    "        \"\"\"\n",
    "        fday_conditions_dict = {1: 'Monday', 2: 'Tuesday', 3:'Wednesday',\n",
    "                                4: 'Thursday', 5: 'Friday'}\n",
    "        \n",
    "        return fday_conditions_dict[individual_day]\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def map_element_to_stage(individual_stage, stage_type):\n",
    "        \"\"\"\n",
    "        Type: static method\n",
    "\n",
    "        Input: 1. individual_stage: Is the element that we want to find a t/n stage for.\n",
    "               2. stage_type: is the type of stage (n_stage or t_stage)\n",
    "\n",
    "        Explanation: this static method get an element and find a proper stage.\n",
    "\n",
    "        Output: 1. is a stage.\n",
    "        \"\"\"\n",
    "        if stage_type == 'n_stage':\n",
    "            stage_conditions_dict = {('N0'): 'N0', ('N1'): 'N1', ('N2','N2a', 'N2b', 'N2c'): 'N2',\n",
    "                                  ('N3'): 'N3'}\n",
    "\n",
    "        else:\n",
    "            stage_conditions_dict = {('Tis', 'T0', 'T1'): 'T1', ('T2'): 'T2', ('T3'): 'T3',\n",
    "                                  ('T4a', 'T4b'): 'T4'}\n",
    "\n",
    "        for key in stage_conditions_dict.keys():\n",
    "            try:\n",
    "                if individual_stage in key:\n",
    "                    return stage_conditions_dict[key]\n",
    "            \n",
    "            except TypeError:\n",
    "                return 'None'   \n",
    "\n",
    "    @staticmethod\n",
    "    def map_element_to_xer(individual_xer):\n",
    "        \"\"\"\n",
    "        Type: static method\n",
    "\n",
    "        Input: 1. individual_xer: Is the element that we want to find a xerostomia label for.\n",
    "\n",
    "        Explanation: this static method get an element and find a xerostomia endpoint label.\n",
    "\n",
    "        Output: 1. is a xerostomia endpoint label.\n",
    "        \"\"\"\n",
    "        xer_conditions_dict = {2: 'Positive', 1: 'Negative', 0: 'Not Available'}\n",
    "        return xer_conditions_dict[individual_xer]\n",
    "\n",
    "    @ staticmethod\n",
    "    def map_element_to_trend(individual_trend):\n",
    "        \"\"\"\n",
    "        Type: static method\n",
    "\n",
    "        Input: 1. individual_trend: Is the element that we want to find a xerostomia trend for.\n",
    "\n",
    "        Explanation: this static method get an element and find xerostomia trend.\n",
    "\n",
    "        Output: 1. is a xerostomia trend.\n",
    "        \"\"\"\n",
    "        trend_conditions_dict = {(1, 1):('Negative, Negative'),\n",
    "                                 (2, 1):('Positive, Negative'),\n",
    "                                 (1, 2):('Negative, Positive'),\n",
    "                                 (2, 2):('Positive, Positive')} \n",
    "        \n",
    "        for key in trend_conditions_dict.keys():\n",
    "\n",
    "            if individual_trend[0] == key[0] and individual_trend[1] == key[1]:\n",
    "                return trend_conditions_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPresentor():\n",
    "\n",
    "    def text_home(self):\n",
    "        text = pn.pane.Markdown(\"\"\"\n",
    "                                ## **Introduction**\n",
    "                                This panel is made to explain some of the features of the dataset used in my research.\n",
    "                                It contains one main dataset contains **455** parients. The patients in this dataset can have\n",
    "                                12- or 6- month endpoint for xerostomia. Moreover, 12 month dataset is a subset of the main\n",
    "                                dataset that contains **345** patients whose patients only have 12-month endpoint for xerostomia.\n",
    "                                The patients in the mentioned dataset can have endpoint for 6-month xerostomia. Moreover, the 6-month\n",
    "                                dataset (contains **418** patients) only contains the patients with 6-month xerostomia endpoint who can have\n",
    "                                12-month xerostomia endpoint. Finally, 12-6 month dataset (with **310** patients) contains the patients who\n",
    "                                have both of the endpoints.  \n",
    "\n",
    "                                ## **Features**\n",
    "                                The following features are evaluated in this panel:\n",
    "\n",
    "                                1. **Year**: This bar chart contains the number of patients based on the RT start year.\n",
    "\n",
    "                                2. **First_day**: This bar chart contains the number of patients based on the RT start weekday. \n",
    "\n",
    "                                3. **Number_of_weeklyCTs**: This bar chart investigates for each number of weekly CTs how many patients we have.\n",
    "\n",
    "                                4. **Modality_adjustment**: This bar chart depicts the number of patients with different treatment approaches in each dataset.\n",
    "\n",
    "                                5. **Count_of_weeks**: This bar chart evaluate how many patient we have for each week CT.\n",
    "\n",
    "                                6. **sex**: This bar chart shows the distribution of gender for each cohort.\n",
    "\n",
    "                                7. **tumor_location**: This bar chart presents the distribution of different tumor locations in each dataset.\n",
    "\n",
    "                                8. **age**: This bar chart shows the distribution of patients in different age groups in each dataset.\n",
    "\n",
    "                                9. **n_stage**: This bar chart shows the distribution of different stages of the number of lymph nodes involved in cancer.\n",
    "\n",
    "                                10. **t_stage**: This bar chart shows the distribution of different stages of the cancer.\n",
    "\n",
    "                                11. **xer_06**: This bar chart depicts the number of patients with positive, negative and even without 6-month xerostomia label.\n",
    "\n",
    "                                12. **xer_12**: This bar chart depicts the number of patients with positive, negative and even without 12-month xerostomia label.\n",
    "                                \n",
    "                                13. **xer_trend**: This bar chart presents the trend of the side effect in each patient from 6-month endpoint to 12-month endpoint.\n",
    "                                \"\"\")\n",
    "        return text\n",
    "\n",
    "    def text_total(self):\n",
    "        text = pn.pane.Markdown(\"\"\"\n",
    "                                 ## Explanation\n",
    "                                 This dataset contains **455** patients from which **345** patients have 12-month xerostimia endpoint, and **418** patients have\n",
    "                                 6-month xerostomia endpoint. The extra columns in this bar plot refers to the number of patient who are diagnosed with\n",
    "                                 positive and negative xerostomia 6 months and 12 months after irradiation.\n",
    "                                 \"\"\")\n",
    "        return text \n",
    "\n",
    "    def text_12month(self):\n",
    "        text = pn.pane.Markdown(\"\"\"\n",
    "                                ## Explanation\n",
    "                                This dataset contains **345** patients with 12-month xerostimia endpoint. The most important bar chart for this dataset is **Count_of_weeks**\n",
    "                                since it contains the number of available weekly CTs per week that can be used as an estimation of the number of samples in the dataset to\n",
    "                                train the model.\n",
    "                                 \"\"\")\n",
    "        return text \n",
    "\n",
    "    def text_6month(self):\n",
    "        text = pn.pane.Markdown(\"\"\"\n",
    "                                ## Explanation\n",
    "                                This dataset contains **418** patients with 6-month xerostimia endpoint. This dataset is larger than 6-month dataset, and can be a good starting\n",
    "                                dataset for training different models.The most important bar chart for this dataset is **Count_of_weeks** since it contains the number of available\n",
    "                                weekly CTs per week that can be used as an estimation of the number of samples in the dataset to train the model.\n",
    "                                 \"\"\")\n",
    "        return text\n",
    "    \n",
    "    def text_12_6_month(self):\n",
    "        text = pn.pane.Markdown(\"\"\"\n",
    "                                ## Explanation\n",
    "                                This dataset contains **310** patients with available 6-month and 12-month xerostimia endpoints, which is the smallest dataset. The most important\n",
    "                                feature of this dataset is **xer_trend** bar chart since it depicts the trend ofxerostomia in the patients who have both xerostomia endpoints. As it\n",
    "                                was expected, the number of negative labels are more than positive labels in this dataset.\n",
    "                                 \"\"\")\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dashboard:\n",
    "    '''\n",
    "    This class creates a panel dashboard to which pages can be added\n",
    "    \n",
    "    Arguments: \n",
    "    title (str): title of the dashboard\n",
    "    header_color (str): name of a color or hex color code\n",
    "    css (str): raw css\n",
    "    \n",
    "    Returns:    \n",
    "    dashboard object\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, title: str, header_color: str, css):\n",
    "        # initialise dashboard\n",
    "        self.dashboard = pn.template.BootstrapTemplate(title=title, header_background=header_color, sidebar_width=200)\n",
    "        self.dashboard.main.extend([pn.pane.Markdown(''), pn.Column(width=1000)]) \n",
    "        self.main_page = self.dashboard.main[1]\n",
    "        pn.extension(raw_css=[css])\n",
    "        \n",
    "        # variable to save all the pages\n",
    "        self.pages = {}\n",
    "        \n",
    "        \n",
    "    def add_page(self, title: str, show_page: bool, *contents):\n",
    "        ''' \n",
    "        Adds a page to the dashboards and create a sidebar navigation button for it \n",
    "        \n",
    "        Arguments:\n",
    "        title      (str): title of the page\n",
    "        show_page (bool): boolean to show the page when showing the dahsboard (if more pages have this as True the last page added will be shown)\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "        sidebar_button = pn.widgets.Button(name=title, width=150, css_classes=['sidebar_button'])  # create sidebar button\n",
    "        self.dashboard.sidebar.append(sidebar_button)  # append button to sidebar\n",
    "        sidebar_button.on_click(self._update_page)  # callback\n",
    "        self.pages[title] = [*contents]  # add the contents to the page dictionary\n",
    "        if show_page:\n",
    "            self._show_page(title)\n",
    "    \n",
    "    \n",
    "    def _update_page(self, event):\n",
    "        '''\n",
    "        Private callback method to update the page when a sidebar button is clicked \n",
    "          \n",
    "        Arguments:\n",
    "        event (object): widget cacllback event\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "        name = event.obj.name  # extract name from event\n",
    "        self.main_page.clear()  # clear the main page\n",
    "        self.main_page.append(pn.pane.Markdown(f'# {name}'))  # create title\n",
    "        self.main_page.extend([item for item in self.pages[name]])  # add all of the contents to the page\n",
    "        \n",
    "        \n",
    "    def _show_page(self, title: str):\n",
    "        '''\n",
    "        Private method that show the page of the given page title \n",
    "        \n",
    "        Arguments:\n",
    "        title (str): title of the page\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "        self.main_page.clear()\n",
    "        self.main_page.append(pn.pane.Markdown(f'# {title}'))\n",
    "        self.main_page.extend([item for item in self.pages[title]])\n",
    "            \n",
    "            \n",
    "    def show(self):\n",
    "        '''Shows the dashboard''' \n",
    "        self.dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir('//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART Hooman/WeeklyCT Dataset/Program & Docs/Plotting')\n",
    "# CSS styling\n",
    "css = '''\n",
    ".sidebar_button .bk-btn-group button {\n",
    "    border-radius: 6px;\n",
    "    font-weight: bolder;\n",
    "}\n",
    "\n",
    ".option_button .bk-btn-default.bk-active {\n",
    "    background-color: #00dcff38;\n",
    "    font-weight: bold;\n",
    "    border-color: black;\n",
    "}\n",
    "'''\n",
    "# Text\n",
    "text_obj = TextPresentor()\n",
    "home_text = text_obj.text_home()\n",
    "total_text = text_obj.text_total()\n",
    "twelve_text = text_obj.text_12month()\n",
    "six_text = text_obj.text_6month()\n",
    "twelve_six_text = text_obj.text_12_6_month()\n",
    "\n",
    "print(total_text)\n",
    "\n",
    "# Initialize the Dashboard\n",
    "dataset_db = Dashboard('Dataset', '#00C5CD', css)\n",
    "\n",
    "# Home page\n",
    "dataset_db.add_page('Home', True, home_text)\n",
    "\n",
    "## Total Dataset page\n",
    "# Assign the total dataset and desired columns\n",
    "total_df = pd.read_excel('wk3_12month_df.xlsx').drop(columns=['Unnamed: 0'])\n",
    "column_names_total = ['Year', 'First_day', 'Number_of_weeklyCTs', 'modality_adjusted', 'Count_of_weeks',\n",
    "                      'gender', 'tumor_location', 'age', 'n_stage', 't_stage','xer_06', 'xer_12']\n",
    "\n",
    "# Make the barplot for this dataset\n",
    "bar_plot_total_obj = BarChart(total_df, column_names_total, 'Year')\n",
    "bar_plot_total = bar_plot_total_obj.bar_chart_panel_maker()\n",
    "\n",
    "# Add total dataset page to the panel\n",
    "dataset_db.add_page('Total Datset', False, bar_plot_total, total_text)\n",
    "\n",
    "\n",
    "## 6month Dataset page\n",
    "# Assign the total dataset and desired columns\n",
    "six_month_df = pd.read_excel('Overview_weeklyCT_patients_6month.xlsx').drop(columns=['Unnamed: 0'])\n",
    "column_names_6month = ['Year', 'First_day', 'Number_of_weeklyCTs', 'modality_adjusted', 'Count_of_weeks',\n",
    "                      'gender', 'tumor_location', 'age', 'n_stage', 't_stage']\n",
    "\n",
    "# Make the barplot for this dataset\n",
    "bar_plot_6month_obj = BarChart(six_month_df, column_names_6month, 'Year')\n",
    "bar_plot_6month = bar_plot_6month_obj.bar_chart_panel_maker()\n",
    "\n",
    "# Add 6month dataset page to the panel\n",
    "dataset_db.add_page('6 month Dataset', False, bar_plot_6month, six_text)\n",
    "\n",
    "## 12month Dataset page\n",
    "# Assign the total dataset and desired columns\n",
    "twelve_month_df = pd.read_excel('Overview_weeklyCT_patients_12month.xlsx').drop(columns=['Unnamed: 0'])\n",
    "column_names_12month = ['Year', 'First_day', 'Number_of_weeklyCTs', 'modality_adjusted', 'Count_of_weeks',\n",
    "                      'gender', 'tumor_location', 'age', 'n_stage', 't_stage']\n",
    "\n",
    "# Make the barplot for this dataset\n",
    "bar_plot_12month_obj = BarChart(twelve_month_df, column_names_12month, 'Year')\n",
    "bar_plot_12month = bar_plot_12month_obj.bar_chart_panel_maker()\n",
    "\n",
    "# Add 12month dataset page to the panel\n",
    "dataset_db.add_page('12 Month Dataset', False, bar_plot_12month, twelve_text)\n",
    "\n",
    "## 12- and 6- month Dataset page\n",
    "# Assign the total dataset and desired columns\n",
    "twelve_six_month_df = pd.read_excel('Overview_weeklyCT_patients_12_6_month.xlsx').drop(columns=['Unnamed: 0'])\n",
    "column_names_12_6_month = ['Year', 'First_day', 'Number_of_weeklyCTs', 'modality_adjusted', 'Count_of_weeks',\n",
    "                      'gender', 'tumor_location', 'age', 'n_stage', 't_stage', 'xer_trend']\n",
    "\n",
    "# Make the barplot for this dataset\n",
    "bar_plot_12_6_month_obj = BarChart(twelve_six_month_df, column_names_12_6_month, 'Year')\n",
    "bar_plot_12_6_month = bar_plot_12_6_month_obj.bar_chart_panel_maker()\n",
    "\n",
    "# Add 12-6month dataset page to the panel\n",
    "dataset_db.add_page('12-6 Month Dataset', False, bar_plot_12_6_month, twelve_six_text)\n",
    "\n",
    "\n",
    "dataset_db.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/Users/Hooman Bahrdo/Models/Deep_Learning/DL_NTCP_Xerostomia/datasets/dataset_old_v2/stratified_sampling_test_manual_94.csv'\n",
    "\n",
    "dff = pd.read_csv(path, sep=';').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 1032,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.xer_12.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/Users/Hooman Bahrdo/Deep_learning_datasets/Six_month_final df/datasets/dataset_old_v2/0'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiomics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
