{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, I will describe all the steps that I have taken to make a new dataset for Weekly CTs.\n",
    "\n",
    "Basically, the process contains five different steps:\n",
    "\n",
    "1. Navigation of the folder in which one think there maybe any weeklyCTs. These folders can be on this computer or a user can just make these folders by downloading new patients from MIRADA or other UMCG datasets.\n",
    "\n",
    "2. Extracting only weeklyCTs from these folders and make an excel file from them.\n",
    "\n",
    "3. Transferring the new-founded weeklyCTs into a destination folder (it can be an existing folder for the weeklyCTs or a new folder).\n",
    "\n",
    "4. Making a report excel file of some information about the weeklyCTs in the destination file and some clinical information from the patients who have these weeklyCTs.\n",
    "\n",
    "5. Making a pannel that contains different information about the WeeklyCT dataset.\n",
    "\n",
    "6. A Watchdog is keep the track of all the additions to the destination folder, and save them in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from datetime import time, datetime, date\n",
    "\n",
    "# DICOM Libraries\n",
    "import pydicom as pdcm\n",
    "from pydicom.tag import Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Navigation Phase\n",
    "### DICOM Files\n",
    "All kinds of CTs were stored in the form of DICOM files. DICOM, which stands for Digital Imaging and Communications in Medicine, is a standard for transmitting, storing, and sharing medical images. DICOM files contain information about medical images, such as X-rays, CT scans, MRIs, and ultrasound. This standard ensures the interoperability of medical imaging equipment from different manufacturers. Some key features are:\n",
    "\n",
    "**Metadata:** DICOM files store not only the pixel data of the medical images but also a wealth of metadata. This metadata includes patient information, imaging device details, acquisition parameters, and more.\n",
    "\n",
    "**Interoperability:** DICOM enables the exchange of medical images and related information between different devices and systems. This interoperability is crucial in healthcare settings where various imaging modalities and equipment are used.\n",
    "\n",
    "**Structured Data:** DICOM files use a structured format for information, allowing for consistency and ease of interpretation by different systems. This makes it possible for healthcare professionals to access and understand the data regardless of the equipment used to capture or generate the images.\n",
    "\n",
    "For information of different tags and the definitions one can use the following links: [Wiki](https://en.wikipedia.org/wiki/DICOM), [link](https://dicom.innolitics.com/ciods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_name(image, subf):\n",
    "\n",
    "    # find the name of the folder\n",
    "    try:\n",
    "        folder_name = image[Tag(0x0008103e)].value\n",
    "\n",
    "    except:\n",
    "        study = image[Tag(0x00081030)].value\n",
    "        patient_id = image[Tag(0x00100020)].value\n",
    "        print(f'Warning: folder {study} with {patient_id} ID does NOT have Series Description')\n",
    "        folder_name = subf.split('\\\\')[-1]  \n",
    "\n",
    "    return folder_name\n",
    "\n",
    "def get_patient_id(image):\n",
    "\n",
    "    # Extract the patient ID\n",
    "    try:\n",
    "        patient_id = int(image[Tag(0x00100020)].value)\n",
    "\n",
    "    except:\n",
    "        print(f'Warning: There is NO patient ID')\n",
    "        patient_id = None\n",
    "\n",
    "    return patient_id\n",
    "\n",
    "def get_probable_weklyct_name(name, number, names_list, saver):\n",
    "\n",
    "    lowercase_name = name.lower()\n",
    "\n",
    "    # Search to find 'rct' or 'w' with a number\n",
    "    if ('rct' in lowercase_name or 'w' in lowercase_name) and re.search(r'\\d', name):\n",
    "        saver = name\n",
    "\n",
    "    elif 'wk..' in lowercase_name and not re.search(r'\\d', name):\n",
    "        saver = name\n",
    "\n",
    "    # Check if 'w' is in 'j' and the next element in 'sep_names' is an integer\n",
    "    elif 'w' in lowercase_name and number + 1 < len(names_list) and not re.search(r'\\d', name):\n",
    "\n",
    "        if '2.0' not in names_list[number + 1] and '2,' not in names_list[number + 1]:\n",
    "            saver = name + str(names_list[number + 1])\n",
    "\n",
    "    elif re.search('rct.*[..]|rct.*[#]', lowercase_name) and not re.search(r'\\d', name):\n",
    "        saver = name\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return saver    \n",
    "    \n",
    "def get_hd_fov(name, hd_fov):\n",
    "\n",
    "    lowercase_name = name.lower()\n",
    "    # Search whether there is 'hd' or 'fov' in j\n",
    "    if 'hd' in lowercase_name or 'fov' in lowercase_name:\n",
    "        hd_fov = 1 \n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return hd_fov\n",
    "\n",
    "def get_fraction(name, fraction):\n",
    "\n",
    "    lowercase_name = name.lower()\n",
    "\n",
    "    # Find the fraction number\n",
    "    if 'rct' in lowercase_name and re.search(r'\\d', name):\n",
    "        fraction = int(re.findall(r'\\d+', name)[0])\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return fraction\n",
    "\n",
    "def get_date_information(image):\n",
    "\n",
    "    # Extract the date, the week day, and the week number from study date time\n",
    "    try:\n",
    "        study_datetime_CT = datetime.strptime(image[Tag(0x00080020)].value ,\"%Y%m%d\")\n",
    "        date_info = study_datetime_CT.date()\n",
    "        weekday = study_datetime_CT.weekday() + 1\n",
    "        week_num = study_datetime_CT.isocalendar()[1] #week\n",
    "\n",
    "    except:\n",
    "        date_info = None\n",
    "        weekday = None\n",
    "        week_num = None \n",
    "    \n",
    "    return date_info, weekday, week_num\n",
    "\n",
    "def get_slice_thickness(image):\n",
    "    \n",
    "    # Extract slice thickness\n",
    "    try:\n",
    "        slice_thickness = image['00180050'].value\n",
    "    except:\n",
    "        slice_thickness = None\n",
    "    \n",
    "    return slice_thickness\n",
    "\n",
    "def get_contrast(image):\n",
    "    \n",
    "    # Extract contrast information\n",
    "    try:\n",
    "        image[Tag(0x00180010)].value\n",
    "        contrast=1\n",
    "\n",
    "    except:\n",
    "        contrast=0\n",
    "    \n",
    "    return contrast\n",
    "\n",
    "def get_pixel_spacing(image):\n",
    "\n",
    "    # Extract pixel spacing\n",
    "    try:\n",
    "        pixel_spacing = image[Tag(0x00280030)].value\n",
    "    except:\n",
    "        pixel_spacing = None\n",
    "    \n",
    "    return pixel_spacing\n",
    "\n",
    "def get_ref_uid(image):\n",
    "\n",
    "    # Extract UID\n",
    "    try:\n",
    "        uid = image['00200052'].value\n",
    "    except:\n",
    "        uid = None\n",
    "    \n",
    "    return uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_folder(path_folder, output_path, file_name):\n",
    "\n",
    "    # Add in config\n",
    "    exclusion_set = {'detail', 'ac_ct', 'ld_ct', 'ld ct', 'ac ct'} # CONFIG File\n",
    "    min_slice_num = 50 # CONFIG File\n",
    "    modality = 'CT' # CONFIG File\n",
    "\n",
    "    # Make a group to save all the information\n",
    "    group = list()\n",
    "\n",
    "    for r, d, f in os.walk(path_folder):\n",
    "        # make a list from all the directories \n",
    "        subfolders = [os.path.join(r, folder) for folder in d]\n",
    "\n",
    "        for subf in subfolders:\n",
    "            # number of slices (images) in each DICOM folder, and the name of the folders\n",
    "            slice_num = len(glob.glob(subf+\"/*.DCM\"))\n",
    "\n",
    "            # find whether subf is a path and the number of .DCM images is more than 50\n",
    "            if slice_num > min_slice_num:\n",
    "\n",
    "                # Extract the information of the image \n",
    "                image=pdcm.dcmread(glob.glob(subf+\"/*.DCM\")[0],force=True)\n",
    "                folder_name = get_folder_name(image, subf)\n",
    "    \n",
    "                # Extract the CTs\n",
    "                if image.Modality == modality and all(keyword not in folder_name.lower() for keyword in exclusion_set):\n",
    "   \n",
    "                    patient_id = get_patient_id(image)\n",
    "\n",
    "                    # split the name of the folder into strings of information\n",
    "                    names_list = folder_name.split()\n",
    "\n",
    "                    # Initialize the following three patameters\n",
    "                    saver = None\n",
    "                    hd_fov = 0\n",
    "                    fraction = None\n",
    "\n",
    "                    for number, name in enumerate(names_list):\n",
    "                        saver = get_probable_weklyct_name(name, number, names_list, saver) \n",
    "                        hd_fov = get_hd_fov(name, hd_fov)\n",
    "                        fraction = get_fraction(name, fraction)\n",
    "\n",
    "                    # Find different information\n",
    "                    date_info, weekday, week_num = get_date_information(image)\n",
    "                    slice_thickness = get_slice_thickness(image)\n",
    "                    contrast = get_contrast(image)\n",
    "                    pixel_spacing = get_pixel_spacing(image)\n",
    "                    uid = get_ref_uid(image)\n",
    "\n",
    "                    # Add the information of this group to the total dataset\n",
    "                    group.append({\n",
    "                                'ID': patient_id, 'folder_name': folder_name, 'date': date_info,\n",
    "                                'week_day': weekday, 'week_num': week_num, 'info_header': saver,\n",
    "                                'fraction': fraction, 'HD_FoV': hd_fov, 'slice_thickness': slice_thickness,\n",
    "                                'num_slices': slice_num, 'pixel_spacing': pixel_spacing, 'contrast': contrast,\n",
    "                                'UID': uid, 'path': subf\n",
    "                                })\n",
    "    \n",
    "    # Make a datafrme from the main folder\n",
    "    df = pd.DataFrame(group)\n",
    "\n",
    "    # Save the dataframe\n",
    "    df.to_excel(os.path.join(output_path,file_name), index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1'\n",
    "output_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1'\n",
    "\n",
    "# Correct this one in the main code, this folder have a name tha follow the following structure: 'General_information_{folder_name}.xlsx'\n",
    "file_name = 'General_information_ART_DATA1.xlsx' \n",
    "df = navigate_folder(path_folder, output_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our knowledge about weeklyCTs, we know that they are only available after 2014, so we can just remove the patients before this specific time. Moreover, since this program just navigate all the folders, there may be some duplicated data in those folders, so I need to erase them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    \"\"\"\n",
    "    clean the dataset\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Slice the part of the dataset after the mentioned time.\n",
    "    time_limit = pd.Timestamp('2014-01-01') # CONFIG File\n",
    "    df_copy = df_copy[pd.to_datetime(df_copy.date) > time_limit]\n",
    "\n",
    "    # Drop the doplicated folders\n",
    "    df_copy = df_copy.drop_duplicates(subset=['ID', 'folder_name', 'date'],\n",
    "                                       keep='first', inplace=False, ignore_index=True)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, I will drop all the remained CTs that are not WeeklyCTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_firstday(df, date_list):\n",
    "    try:\n",
    "        first_day = df[df.date == date_list[1]].iloc[0].week_day\n",
    "    except:\n",
    "        first_day = None\n",
    "    \n",
    "    return first_day\n",
    "\n",
    "def find_matching_header(info_headers):\n",
    "    for header in info_headers:\n",
    "        try:\n",
    "            lowercase_header = header.lower()\n",
    "\n",
    "            if any(keyword in lowercase_header for keyword in ['rct', 'w']) and re.search(r'\\d', header):\n",
    "                return header\n",
    "\n",
    "            elif 'wk..' in lowercase_header and not re.search(r'\\d', header):\n",
    "                return header\n",
    "\n",
    "            elif re.search(r'rct.*[..]|rct.*[#]', lowercase_header) and not re.search(r'\\d', header):\n",
    "                return header\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An exception occurred: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_weeklycts_names(df, date_list):\n",
    "\n",
    "    header_list = list()\n",
    "\n",
    "    # Find the headers\n",
    "    for session in date_list[1:]:\n",
    "        info_headers = df[df.date == session].info_header.tolist()\n",
    "        header = find_matching_header(info_headers)\n",
    "\n",
    "        header_list.append(header)\n",
    "\n",
    "    # Ensure the header_list has 9 elements\n",
    "    header_list += [None] * (9 - len(header_list))\n",
    "\n",
    "    return header_list\n",
    "\n",
    "def get_accelerated_rt(patient_id, clinical_df):\n",
    "    try:\n",
    "        accelerated_rt = clinical_df[clinical_df.UMCG==int(patient_id)].Modality_adjusted.values[0]\n",
    "    \n",
    "    except:\n",
    "        accelerated_rt = 'Not Mentioned'\n",
    "    \n",
    "    return accelerated_rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weeklyct_folders(df, output_path):\n",
    "    \"\"\"\n",
    "    This function finds weeklyCTs and drops other types of CTs\n",
    "    \"\"\"\n",
    "    # Call clinical df to extract Accelerated program for each patient\n",
    "    clinical_df_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART Hooman/Xerostomia_dataset.xlsx' # CONFIG File\n",
    "    clinical_df = pd.read_excel(clinical_df_path)\n",
    "\n",
    "    group = list()\n",
    "\n",
    "    # Separate each ID dataframe\n",
    "    id_df = pd.DataFrame(df.groupby(['ID']))\n",
    "\n",
    "    for counter, id_num in enumerate(id_df[0]):\n",
    "\n",
    "        df = id_df[1][counter]\n",
    "\n",
    "        # Extract the parts suspected to contain weeklyCTs\n",
    "        df = df[(df['folder_name'].str.lower().str.contains('rct') & (df['date'] != df['date'].min())) \\\n",
    "                | ((df['date'] == df['date'].min()))]\n",
    "       \n",
    "        date_list = sorted(list(df.date.unique())) # Find the list of dates\n",
    "        rtstart = date_list[0] # Extract RTSTART  \n",
    "        first_day = get_firstday(df, date_list) # the week day of the first treatment\n",
    "\n",
    "        # Extract the weeklyCTs names and first day of the treatment\n",
    "        header_list= get_weeklycts_names(df, date_list)\n",
    "\n",
    "        # Extract other parameters\n",
    "        durations = date_list[1:]\n",
    "        weekly_ct_num = len(durations)       \n",
    "        durations += [None] * (9 - len(durations)) # Ensure it has 9 elements\n",
    "        Modality_adjusted = get_accelerated_rt(id_num, clinical_df)\n",
    "\n",
    "        group.append({'ID': int(id_num), 'Baseline': rtstart, 'Session1': durations[0],\n",
    "                        'Session2': durations[1], 'Session3': durations[2],'Session4': durations[3],\n",
    "                        'Session5': durations[4], 'Session6': durations[5],'Session7': durations[6],\n",
    "                        'Session8': durations[7],'Session9': durations[8], 'Fraction1': header_list[0],\n",
    "                        'Fraction2': header_list[1], 'Fraction3': header_list[2],'Fraction4': header_list[3],\n",
    "                        'Fraction5': header_list[4], 'Fraction6': header_list[5], 'Fraction7': header_list[6],\n",
    "                        'Fraction8': header_list[7],'Fraction9': header_list[8], 'First_day': first_day,\n",
    "                        'Number_of_CTs': df.shape[0], 'Number_of_weeklyCTs': weekly_ct_num, 'modality_adjusted':Modality_adjusted})\n",
    "        \n",
    "    # Make a datafrme from the main folder\n",
    "    df_final = pd.DataFrame(group)\n",
    "\n",
    "    # Drop the patients who does not have weeklyCTs\n",
    "    df_final = df_final[~(df_final.Number_of_weeklyCTs == 0)]\n",
    "    df_final = df_final.reset_index().drop(columns=['index'])\n",
    "\n",
    "    # Save the dataframe\n",
    "    # df_final.to_csv(os.path.join(output_path, file_name), index=False)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly_file_name = 'weeklyct_output.csv' DO NOT NEED THIS ONE, IT IS A MIDDLE PROCESS\n",
    "weeklyct_df = extract_weeklyct_folders(df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of Navigation Phase, I will replace all the strings in the header part of the dataframe into fraction numbers. There are multiple conditions here. some patients have their own fractions in their headers e.g. 'rct13', but some others have week number like 'wk3' or have a part of the repeated CT name such as 'rct..', 'wk', 'wk..', and so on. for the first group, I just use the number of fractions in the header. However, for the second and third group, I calculate the probable numeber of fractions using the following criteria.\n",
    "if the patient has accelarated RT plan, I assume that they should get 1.2 fraction per day (only in working days), so it mean 6 fractions per week.Ans, for patients with other types of the treatment, I suppose that they  should get 1 fraction per working day, so in total 5 per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to extract numbers only if 'wk' is not present\n",
    "def extract_numbers(text):\n",
    "    if isinstance(text, str) and 'wk' not in text and re.search(r'\\d', text):\n",
    "        \n",
    "        return  float(''.join(filter(str.isdigit, text)))       \n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def get_existing_fractions(df):\n",
    "    \"\"\"\n",
    "    This function extract all the fractions exist in the data itself.\n",
    "    \"\"\"\n",
    "    for header in df.iloc[:, 11:20].columns:\n",
    "        df[header] = df[header].apply(extract_numbers)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_coef(Modality_adjusted):\n",
    "    \"\"\"\n",
    "    Get the coefficient of the fractions\n",
    "    \"\"\"\n",
    "    accelerated_list = ['Accelerated RT', 'Bioradiation'] # CONFIG File\n",
    "    not_accelerated_list = ['Chemoradiation', 'Conventional RT'] # CONFIG File\n",
    "    \n",
    "    if Modality_adjusted in not_accelerated_list:\n",
    "        coef = 1.0\n",
    "    \n",
    "    elif Modality_adjusted in accelerated_list:\n",
    "        coef = 1.2\n",
    "\n",
    "    else:\n",
    "        coef = 0.0\n",
    "\n",
    "    return coef\n",
    "\n",
    "def calculate_fraction(raw, fraction, fraction_num, coef, counter):\n",
    "    try:\n",
    "    \n",
    "        if isinstance(fraction, str) and 'wk' in fraction and  counter == 0:\n",
    "            fraction_num = (len(pd.bdate_range( raw[f'Baseline'], raw[f'Session{1}'])) - 1) * coef + 1\n",
    "\n",
    "        elif isinstance(fraction, str) and 'wk' in fraction and  counter != 0:\n",
    "            fraction_num += (len(pd.bdate_range( raw[f'Session{counter}'], raw[f'Session{counter+1}'])) - 1) * coef\n",
    "                \n",
    "        elif isinstance(fraction, str) and 'wk' not in fraction and not re.search(r'\\d', fraction) and counter==0:\n",
    "            fraction_num += (len(pd.bdate_range( raw[f'Baseline'], raw[f'Session{1}'])) - 1) * coef + 1\n",
    "\n",
    "        # This part does not work  if the rct.. or rct# is seperated from other part\n",
    "        elif isinstance(fraction, str) and 'wk' not in fraction and not re.search(r'\\d', fraction) and counter!=0:\n",
    "            fraction_num += (len(pd.bdate_range( raw[f'Session{counter}'], raw[f'Session{counter+1}'])) - 1) * coef\n",
    "\n",
    "        elif fraction is np.nan and counter < raw.Number_of_weeklyCTs and counter==0:\n",
    "            fraction_num = (len(pd.bdate_range( raw[f'Baseline'], raw[f'Session{1}'])) - 1) * coef + 1\n",
    "\n",
    "        elif fraction is np.nan and counter < raw.Number_of_weeklyCTs and counter!=0:\n",
    "            fraction_num += (len(pd.bdate_range( raw[f'Session{counter}'], raw[f'Session{counter+1}'])) - 1) * coef              \n",
    "\n",
    "        elif isinstance(fraction, int) or isinstance(fraction, float):\n",
    "            fraction_num = fraction\n",
    "\n",
    "        else:\n",
    "            fraction_num = None\n",
    "        return fraction_num \n",
    "\n",
    "    except:\n",
    "        return fraction_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fractions(df, output_path, file_name):\n",
    "    \"\"\"\n",
    "    This function finds or calculates all the fractions\n",
    "    \"\"\"\n",
    "    # Make a copy of the dataset\n",
    "    df_copy = df.copy()\n",
    "    coef_list = list()\n",
    "    # Find all the existing fractions in the dataset\n",
    "    df_copy = get_existing_fractions(df_copy)\n",
    "\n",
    "    # Iterate through patients\n",
    "    for index, raw in df_copy.iterrows():\n",
    "\n",
    "        fraction_list = list()\n",
    "        fraction_num = 0\n",
    "\n",
    "        # Calculate the coefficient\n",
    "        coef = get_coef(raw.Modality_adjusted)\n",
    "\n",
    "        # Iterate through fractions\n",
    "        for counter, fraction in enumerate(raw.iloc[11:20]):\n",
    "\n",
    "            # Calculate and add different fractions to the list of fractions\n",
    "            fraction_num = calculate_fraction(raw, fraction, fraction_num, coef, counter)\n",
    "            fraction_list.append(fraction_num)\n",
    "\n",
    "        df_copy.iloc[index, 11:20] = fraction_list\n",
    "        coef_list.append(coef)\n",
    " \n",
    "    df_copy['Coefficient'] = coef_list\n",
    "\n",
    "    # Save the dataframe\n",
    "    df_copy.to_excel(os.path.join(output_path, file_name), index=False)\n",
    "\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct this one in the main code, this folder have a name tha follow the following structure: 'WeeklyCTs_fraction_{folder_name}.xlsx'\n",
    "file_name = 'WeeklyCTs_fraction_ART_DATA1.xlsx'\n",
    "weeklyct_df = add_fractions(weeklyct_df, output_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the first phase can be extracting the information of a specific week e.g. week3. To achieve this aim, I will make a function, that can be call and return an excel file for patients who have a specific week fraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_week_information(main_df, weeklyct_df, week_name):\n",
    "\n",
    "    accelerated_list = ['Accelerated RT', 'Bioradiation'] # CONFIG File\n",
    "    not_accelerated_list = ['Chemoradiation', 'Conventional RT'] # CONFIG File\n",
    "    fraction_range_dict = {'week1': {'not_accelerated':[0.0, 5.0], 'accelerated': [0.0, 6.0]}, # Config File\n",
    "                           'week2': {'not_accelerated':[5.0, 10.0], 'accelerated': [6.0, 12.0]},\n",
    "                           'week3': {'not_accelerated':[10.0, 15.0], 'accelerated': [12.0, 18.0]},\n",
    "                           'week4': {'not_accelerated':[15.0, 20.0], 'accelerated': [18.0, 24.0]},\n",
    "                           'week5': {'not_accelerated':[20.0, 25.0], 'accelerated': [24.0, 30.0]},\n",
    "                           'week6': {'not_accelerated':[25.0, 30.0], 'accelerated': [30.0, 36.0]},\n",
    "                           'week7': {'not_accelerated':[30.0, 35.0], 'accelerated': [36.0, 42.0]},\n",
    "                           'week8': {'not_accelerated':[35.0, 40.0], 'accelerated': [42.0, 48.0]}}\n",
    "    week_list = list()\n",
    "\n",
    "    # Iterate through patients\n",
    "    for _, raw in weeklyct_df.iterrows():\n",
    "        matching_list = []\n",
    "        fraction_seri = raw.iloc[11:20]\n",
    "        #print(raw.modality_adjusted)\n",
    "        # Find any columns that have values inside the range of a a specific week\n",
    "        if raw.modality_adjusted in not_accelerated_list:\n",
    "            matching_list = [column for column in fraction_seri.index \\\n",
    "            if (raw[column]is not None and raw[column] > fraction_range_dict[week_name]['not_accelerated'][0] \\\n",
    "                and raw[column] <= fraction_range_dict[week_name]['not_accelerated'][1])]\n",
    "\n",
    "        elif raw.modality_adjusted in accelerated_list:\n",
    "            matching_list = [column for column in fraction_seri.index \\\n",
    "            if (raw[column]is not None and raw[column] > fraction_range_dict[week_name]['accelerated'][0] \\\n",
    "                and raw[column] <= fraction_range_dict[week_name]['accelerated'][1])]\n",
    "        # print(matching_list)\n",
    "        # If finds a column, add some information of  that patient to the dictionary\n",
    "        if len(matching_list) > 0:\n",
    "            for matched_fraction in matching_list:\n",
    "                week_num = matched_fraction[-1]\n",
    "                week_list.append({'ID': raw.ID,\n",
    "                                 'date': raw[f'Session{week_num}'],\n",
    "                                 'treatment_week': week_name,\n",
    "                                 'Fraction_num': matched_fraction, \n",
    "                                 'Fraction_magnitude': raw[matched_fraction], \n",
    "                                 'modality_adjusted': raw.modality_adjusted})\n",
    "            # print(week_list)\n",
    "    # Make a datafrme from the main folder\n",
    "    week_df = pd.DataFrame(week_list)\n",
    "    final_df = week_df.merge(main_df, on=['ID', 'date']).drop(columns=['fraction'])\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_df = get_a_week_information(df, weeklyct_df, 'week6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Phase\n",
    "In this phase, dataframes from different folders (it can be one or more folders) gather to gether to make a total dataframe for all the dataset in different folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(name):\n",
    "\n",
    "    output_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1' # CONFIG File\n",
    "\n",
    "    try:\n",
    "        # If the file is an excel file\n",
    "        if '.xlsx' in name:\n",
    "            df = pd.read_excel(os.path.join(output_path, name))\n",
    "            \n",
    "        # If the file is a csv file\n",
    "        elif '.csv' in name:\n",
    "            df = pd.read_csv(os.path.join(output_path, name)) # Comma seperated\n",
    "\n",
    "            # If the csv file is semi-colon seperated\n",
    "            if ';' in df.columns[0]:\n",
    "                df = pd.read_csv(os.path.join(output_path, name), sep=';')\n",
    "\n",
    "        # Erase the index columns if there is any\n",
    "        if any('unnamed' in col_name.lower() for col_name in df.columns):\n",
    "            excess_column_names = [col_name for col_name in df.columns if 'unnamed' in col_name.lower()]\n",
    "            df = df.drop(columns=excess_column_names)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'Warning: file {name} was not found')\n",
    "    \n",
    "    except ValueError:\n",
    "        print(f'File {name} is not supported by this program.')\n",
    "\n",
    "\n",
    "def concat_dataframes(df_name_list):\n",
    "    \"\"\"\n",
    "    This function accepts excel and csv files. csvs can be comma-seperated or semicolon-seperated\n",
    "    \"\"\"\n",
    "    # Make an empty df to gather all of the dataframes here.\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for name in df_name_list:\n",
    "        df = read_dataframe(name)\n",
    "\n",
    "        try:\n",
    "           final_df = pd.concat([final_df, df], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'ERROR:error {e} ocurs for {name} folder')\n",
    "            pass\n",
    "\n",
    "    # Drop duplicated patients\n",
    "    if 'weeklyct' in df_name_list[0].lower(): \n",
    "        final_df = final_df.drop_duplicates(subset=['ID'])\n",
    "\n",
    "    # Reset the index\n",
    "    final_df = final_df.sort_values('ID').reset_index().drop(columns=['index'])\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, I will use the above functions to make two final datasets: WeeklyCT_dataset and General_dataset.\n",
    "\n",
    "### WeeklyCT Final Dataframe\n",
    "This dataset contains clinical and some technical information about the patients who have WeeklyCTs. This dataset will be used further in plotting phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataframes(desired_file):\n",
    "    \"\"\"\n",
    "    This function makes the list of the desired file names. It can be weeklyCT files or General files\n",
    "    \"\"\"\n",
    "    output_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1' # CONFIG File\n",
    "\n",
    "    # Find all the relavant dataframes\n",
    "    file_list = os.listdir(output_path)\n",
    "    desired_file_list = [file_name for file_name in file_list if desired_file in file_name.lower()]\n",
    "\n",
    "    return desired_file_list\n",
    "\n",
    "def call_clinical_dataframe():\n",
    "    clinical_df_name = 'Xerostomia_dataset.xlsx' # CONFIG File\n",
    "\n",
    "    # Define a mapping between source and target column names\n",
    "    column_mapping = {'UMCG': 'ID', # CONFIG File\n",
    "                      'GESLACHT': 'gender', \n",
    "                      'LEEFTIJD': 'age',\n",
    "                      'Loctum2': 'tumor_location',\n",
    "                      'N_stage': 'n_stage',\n",
    "                      'TSTAD_DEF': 't_stage',\n",
    "                      'HN35_Xerostomia_M06': 'xer_06',\n",
    "                      'HN35_Xerostomia_M12': 'xer_12'}   \n",
    "    \n",
    "    clinical_df = read_dataframe(clinical_df_name)\n",
    "    desired_column_list = list(column_mapping.keys())\n",
    "\n",
    "    # Slice the desired part\n",
    "    clinical_df = clinical_df.loc[:,desired_column_list]\n",
    "\n",
    "    # Map the name of the columns to the desired names\n",
    "    clinical_df = clinical_df.rename(columns=column_mapping)\n",
    "\n",
    "    return clinical_df\n",
    "\n",
    "\n",
    "def make_weeklyct_dataframe():\n",
    "    \"\"\"\n",
    "    This function makes the final weeklyCT dataframe\n",
    "    \"\"\"\n",
    "    make_label_df = True # Config File\n",
    "    label_list = ['xer_06_y', 'xer_12_y'] # Config File\n",
    "\n",
    "    file_names = find_dataframes('weeklyct')\n",
    "    df = concat_dataframes(file_names)\n",
    "    clinical_df = call_clinical_dataframe()\n",
    "    final_weeklyct_df = df.merge(clinical_df, on='ID')\n",
    "\n",
    "    # Save the dataframe\n",
    "    final_weeklyct_df.to_excel(os.path.join(output_path, 'Overview_weeklyCT_patients.xlsx'), index=False)\n",
    "\n",
    "    # If dataframe based on labels is needed\n",
    "    if make_label_df:\n",
    "        for label in label_list:\n",
    "            label_df = final_weeklyct_df[final_weeklyct_df[label].notnull()]\n",
    "            label_df.to_excel(os.path.join(output_path, f'Overview_weeklyCT_patients_{label}.xlsx'), index=False)\n",
    "\n",
    "    return final_weeklyct_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df = make_weeklyct_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Dataframe\n",
    "This dataframe contains the information of the available weeklyCT folder. This dataframe will be used further in transferring phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_general_dataframe():\n",
    "    week_list = ['week1', 'week2', 'week3', 'week4', 'week5', 'week6', 'week7', 'week8'] # CONFIG File (It can be week dictionary key list)\n",
    "    output_path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/OPC_data/ART_DATA1' # CONFIG File\n",
    "\n",
    "    # Make a dataframe from all the general files\n",
    "    file_names = find_dataframes('general')\n",
    "    general_df = concat_dataframes(file_names)\n",
    "    weekly_df = pd.read_excel(os.path.join(output_path, 'Overview_weeklyCT_patients.xlsx'))\n",
    "\n",
    "    final_general_df = pd.DataFrame()\n",
    "\n",
    "    # Make the datframe for each week and concat all of them to make a dataset\n",
    "    for week_name in week_list:\n",
    "        week_df = get_a_week_information(general_df, weekly_df, week_name)\n",
    "        final_general_df = pd.concat([final_general_df, week_df], ignore_index=True)\n",
    "    \n",
    "    # Sort the dataset based on ID\n",
    "    final_general_df = final_general_df.sort_values('ID').reset_index().drop(columns=['index'])\n",
    "    # Save the dataframe\n",
    "    final_general_df.to_excel(os.path.join(output_path, 'General_information.xlsx'), index=False)\n",
    "\n",
    "    return final_general_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_general_df = make_general_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferring Phase\n",
    "In this phase, all the new weeklyCTs will be transferred into the determined and final folder. If there is the same weeklyCT with the same Patient ID, fraction and week number, this program skips that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/Users/Hooman Bahrdo/Models/Deep_Learning/DL_NTCP_Xerostomia/datasets/dataset_old_v2/stratified_sampling_test_manual_94.csv'\n",
    "\n",
    "dff = pd.read_csv(path, sep=';').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 1032,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.xer_12.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '//zkh/appdata/RTDicom/Projectline_HNC_modelling/Users/Hooman Bahrdo/Deep_learning_datasets/Six_month_final df/datasets/dataset_old_v2/0'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiomics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
